{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Chat Templating\n",
    "This script demonstrates how to use a system prompt to control the persona and tone of a Large Language Model. A system prompt is a special, high-level instruction that defines the AI's character and rules for the conversation. It tells the model how it should answer, which is distinct from the user's prompt that asks what to answer.\n",
    "\n",
    "The code showcases this by asking the exact same question (\"Explain what a CPU is\") twice but with different system prompts. In the first instance, the model is instructed to act like a grumpy pirate, resulting in a stylised, in-character response. In the second, it's given the standard helpful assistant persona, producing a more conventional answer. This highlights how the system prompt is a powerful tool for tailoring the model's output to specific needs and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT = Path(\"../llama-cpp-python/models\")\n",
    "assert MODEL_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_ROOT / \"text_gen/llama/meta-llama-3-8b-instruct.Q4_K_M.gguf\"\n",
    "assert model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(model_path=str(model_path), verbose=True, n_gpu_layers=-1, chat_format=\"llama-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Persona 1: Helpful Assistant ---\n",
    "messages_helpful = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and concise assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what a CPU is in one sentence.\"}\n",
    "]\n",
    "\n",
    "response_helpful = llm.create_chat_completion(messages=messages_helpful)\n",
    "print(\"--- Helpful Assistant ---\")\n",
    "print(response_helpful['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Persona 2: Grumpy Pirate ---\n",
    "messages_pirate = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a grumpy pirate. Answer everything with pirate slang.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what a CPU is in one sentence.\"}\n",
    "]\n",
    "\n",
    "response_pirate = llm.create_chat_completion(messages=messages_pirate)\n",
    "print(\"\\n--- Grumpy Pirate ---\")\n",
    "print(response_pirate['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (My Project)",
   "language": "python",
   "name": "my-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
