{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "This script demonstrates how to visualise the semantic relationships between words using word embeddings and Principal Component Analysis (PCA). First, each word from the input list is converted into a high-dimensional numerical vector (an embedding) using a Large Language Model. These embeddings capture the word's meaning, so words with similar contexts are represented by similar vectors. Because it's impossible to plot data with thousands of dimensions, PCA is then used to compress this complex information into just two dimensions. The final scatter plot displays these 2D representations, visually confirming the model's understanding of language by showing that words with similar meanings (e.g., all the fruits or all the animals) appear clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT = Path(\"../llama-cpp-python/models\")\n",
    "assert MODEL_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_ROOT / \"text_gen/bert-base-uncased-Q8_0.gguf\"\n",
    "assert model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(model_path=str(model_path), embedding=True, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = llm.create_embedding(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embeddings[\"data\"][0][\"embedding\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_and_phrases = [\n",
    "    # Fruits\n",
    "    \"apple\", \"banana\", \"orange\", \"strawberry\", \"grape\",\n",
    "    # Animals\n",
    "    \"dog\", \"cat\", \"lion\", \"tiger\", \"elephant\",\n",
    "    # Technology\n",
    "    \"computer\", \"smartphone\", \"internet\", \"keyboard\", \"software\",\n",
    "    # Weather\n",
    "    \"rain\", \"sunshine\", \"snow\", \"wind\", \"cloud\",\n",
    "    # Musical Instruments\n",
    "    \"guitar\", \"piano\", \"violin\", \"drums\", \"trumpet\",\n",
    "    # Clothing\n",
    "    \"shirt\", \"trousers\", \"jacket\", \"shoes\", \"hat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = [] # Create an empty list to store the results\n",
    "\n",
    "# Loop through each item in your list\n",
    "for item in words_and_phrases:\n",
    "    # Get the embedding for a single item\n",
    "    embedding_data = llm.create_embedding(item)\n",
    "\n",
    "    # Extract the vector and append it to our list\n",
    "    embedding_vector = np.array(embedding_data['data'][0]['embedding'])\n",
    "    all_embeddings.append(embedding_vector.mean(0))\n",
    "\n",
    "# Convert the list of embeddings into a single NumPy array after the loop\n",
    "embeddings = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Embeddings generated with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Apply PCA to Reduce Dimensions ---\n",
    "# We are reducing the ~4096 dimensions of the embeddings down to 2.\n",
    "print(\"--- Applying PCA to reduce dimensions to 2D ---\")\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "print(f\"Embeddings reduced to shape: {embeddings_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Plot the Results ---\n",
    "print(\"--- Plotting results ---\")\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create a scatter plot of the 2D embeddings\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "\n",
    "# Add labels to each point on the plot\n",
    "for i, word in enumerate(words_and_phrases):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)\n",
    "\n",
    "plt.title('2D PCA of Word and Phrase Embeddings', fontsize=16)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
