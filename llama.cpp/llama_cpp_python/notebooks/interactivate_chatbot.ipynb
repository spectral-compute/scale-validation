{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Interactive Chatbot\n",
    "\n",
    "This notebook demonstrates the core mechanics of building a stateful, multi-turn chatbot. The key to enabling a continuous conversation is the conversation history, a list that stores every user message and assistant response. On each turn, this history is sent back to the model, providing the necessary context to understand follow-up questions and maintain a coherent dialogue.\n",
    "\n",
    "To prevent the conversation from exceeding the model's context window, a function (manage_conversation_history) trims the oldest messages, ensuring the prompt remains within a manageable size. A crucial feature for user experience is response streaming (stream=True). Instead of waiting for the entire reply, the code processes and prints each piece (or token) of the response as it's generated, creating the familiar real-time \"typing\" effect. The script simulates a conversation by iterating through predetermined inputs, continuously updating the history with both the user's turn and the model's full reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT = Path(\"../llama-cpp-python/models\")\n",
    "assert MODEL_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_ROOT / \"text_gen/llama/meta-llama-3-8b-instruct.Q4_K_M.gguf\"\n",
    "assert model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(\n",
    "    model_path=str(model_path),\n",
    "    chat_format=\"llama-3\",  # The chat format MUST match the model\n",
    "    n_ctx=4096,           # Context window size\n",
    "    n_gpu_layers=-1,      # Use -1 to offload all layers to GPU\n",
    "    verbose=True         # Set to True to see llama.cpp logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_conversation_history(history, max_turns):\n",
    "    \"\"\"\n",
    "    Manages the conversation history to fit within a sliding window.\n",
    "\n",
    "    This function keeps the system prompt and the most recent 'max_turns'\n",
    "    of the user-assistant conversation.\n",
    "\n",
    "    Args:\n",
    "        history (list): The list of conversation messages.\n",
    "        max_turns (int): The maximum number of conversational turns to keep.\n",
    "\n",
    "    Returns:\n",
    "        list: The trimmed conversation history.\n",
    "    \"\"\"\n",
    "    # A turn consists of one user message and one assistant message (2 items)\n",
    "    max_history_items = max_turns * 2\n",
    "\n",
    "    # Always keep the first message, which is the system prompt\n",
    "    system_prompt = history[0]\n",
    "\n",
    "    # Get the conversational part of the history (all but the system prompt)\n",
    "    conversation = history[1:]\n",
    "\n",
    "    # If the conversation is longer than the max allowed, trim it\n",
    "    if len(conversation) > max_history_items:\n",
    "        # Keep only the last 'max_history_items' messages\n",
    "        trimmed_conversation = conversation[-max_history_items:]\n",
    "        # Reconstruct the history with the system prompt and the trimmed conversation\n",
    "        return [system_prompt] + trimmed_conversation\n",
    "\n",
    "    # If not longer, return the original history\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "def get_config():\n",
    "    # Set the maximum number of conversation turns to remember\n",
    "    MAX_TURNS = 10\n",
    "\n",
    "    # 2. Set up the conversation history\n",
    "    conversation_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and friendly assistant. Always be polite and concise.\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return MAX_TURNS, conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we run an predetermined input to show you how it works\n",
    "MAX_TURNS, conversation_history = get_config()\n",
    "predetermined_inputs = [\n",
    "    \"What is the capital of the United Kingdom?\",\n",
    "    \"What is a famous landmark there that involves a clock?\",\n",
    "    \"How old is that landmark?\",\n",
    "    \"Thank you for the information.\"\n",
    "]\n",
    "\n",
    "print(f\"--- Starting automated chat with {len(predetermined_inputs)} inputs ---\")\n",
    "\n",
    "# 2. Loop through the predetermined inputs instead of waiting for user input\n",
    "for user_input in predetermined_inputs:\n",
    "    # Print the user's turn to simulate a conversation\n",
    "    print(f\"You: {user_input}\")\n",
    "\n",
    "    # Add the new user message to the full history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Manage the history BEFORE sending it to the model\n",
    "    history_for_model = manage_conversation_history(conversation_history, MAX_TURNS)\n",
    "\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    # 4. Generate and stream the response\n",
    "    response_stream = llm.create_chat_completion(\n",
    "        messages=history_for_model,  # Use the potentially trimmed history\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if \"content\" in delta:\n",
    "            token = delta[\"content\"]\n",
    "            print(token, end=\"\", flush=True)\n",
    "            full_response += token\n",
    "\n",
    "    # Print a newline for better formatting between turns\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 5. Add the assistant's full response to the original, untrimmed history\n",
    "    if full_response:\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "print(\"--- Automated chat finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (My Project)",
   "language": "python",
   "name": "my-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
