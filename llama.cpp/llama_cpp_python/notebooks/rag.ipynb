{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances a Large Language Model's (LLM) ability to answer questions by providing it with relevant, up-to-date information from an external knowledge source. Instead of relying solely on the model's pre-trained knowledge, RAG first retrieves relevant documents and then uses that information to augment the prompt, allowing the LLM to generate a more accurate and contextually grounded response.\n",
    "\n",
    "In this notebook, we are performing the fundamental Retrieval step of RAG. We convert the user's query and a collection of documents into numerical representations called embeddings. By calculating the cosine similarity between the query's embedding and each document's embedding, we can mathematically identify and retrieve the document that is most semantically similar to the user's question. This retrieved text then serves as the specific context for the LLM to generate its final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT = Path(\"../llama-cpp-python/models\")\n",
    "assert MODEL_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_ROOT / \"text_gen/llama/llama-2-7b.Q4_0.gguf\"\n",
    "assert model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(\n",
    "    model_path=str(model_path),\n",
    "    embedding=True,  # Enable embedding generation\n",
    "    n_ctx=2048,  # Set context size\n",
    "    verbose=True,\n",
    "    n_gpu_layers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The new Orion spacecraft is designed for deep-space missions to the Moon and Mars.\",\n",
    "    \"The James Webb Space Telescope allows us to see the first galaxies ever formed.\",\n",
    "    \"Big Ben is the nickname for the Great Bell of the striking clock at the north end of the Palace of Westminster.\",\n",
    "    \"Hitchin is a market town in the North Hertfordshire district in Hertfordshire, England.\",\n",
    "]\n",
    "\n",
    "# Create embeddings for each document\n",
    "doc_embeddings = [llm.embed(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Where is Hitchin?\"\n",
    "query_embedding = llm.embed(user_query)\n",
    "\n",
    "# Simple cosine similarity search\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "similarities = [cosine_similarity(np.array(query_embedding).mean(0), np.array(doc_emb).mean(0)) for doc_emb in doc_embeddings]\n",
    "most_relevant_doc_index = np.argmax(similarities)\n",
    "retrieved_context = documents[most_relevant_doc_index]\n",
    "\n",
    "print(f\"üîç Most relevant document found: '{retrieved_context}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (My Project)",
   "language": "python",
   "name": "my-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
